{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from lib.ops import *\n",
    "from lib.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    model_name = \"GAN\"     # name for checkpoint\n",
    "\n",
    "    def __init__(self, sess, epoch, batch_size, z_dim, dataset_name, checkpoint_dir, result_dir, log_dir):\n",
    "        self.sess = sess\n",
    "        self.dataset_name = dataset_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.result_dir = result_dir\n",
    "        self.log_dir = log_dir\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if dataset_name == 'mnist' or dataset_name == 'fashion-mnist':\n",
    "            # parameters\n",
    "            self.input_height = 28\n",
    "            self.input_width = 28\n",
    "            self.output_height = 28\n",
    "            self.output_width = 28\n",
    "\n",
    "            self.z_dim = z_dim         # dimension of noise-vector\n",
    "            self.c_dim = 1\n",
    "\n",
    "            # train\n",
    "            self.learning_rate = 0.0002\n",
    "            self.beta1 = 0.5\n",
    "\n",
    "            # test\n",
    "            self.sample_num = 64  # number of generated images to be saved\n",
    "\n",
    "            # load mnist\n",
    "            self.data_X, self.data_y = load_mnist(self.dataset_name)\n",
    "\n",
    "            # get number of batches for a single epoch\n",
    "            self.num_batches = len(self.data_X) // self.batch_size\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def discriminator(self, x, is_training=True, reuse=False):\n",
    "        # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "        # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n",
    "        with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "\n",
    "            net = lrelu(conv2d(x, 64, 4, 4, 2, 2, name='d_conv1'))\n",
    "            net = lrelu(bn(conv2d(net, 128, 4, 4, 2, 2, name='d_conv2'), is_training=is_training, scope='d_bn2'))\n",
    "            net = tf.reshape(net, [self.batch_size, -1])\n",
    "            net = lrelu(bn(linear(net, 1024, scope='d_fc3'), is_training=is_training, scope='d_bn3'))\n",
    "            out_logit = linear(net, 1, scope='d_fc4')\n",
    "            out = tf.nn.sigmoid(out_logit)\n",
    "\n",
    "            return out, out_logit, net\n",
    "\n",
    "    def generator(self, z, is_training=True, reuse=False):\n",
    "        # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "        # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n",
    "        with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "            net = tf.nn.relu(bn(linear(z, 1024, scope='g_fc1'), is_training=is_training, scope='g_bn1'))\n",
    "            net = tf.nn.relu(bn(linear(net, 128 * 7 * 7, scope='g_fc2'), is_training=is_training, scope='g_bn2'))\n",
    "            net = tf.reshape(net, [self.batch_size, 7, 7, 128])\n",
    "            net = tf.nn.relu(\n",
    "                bn(deconv2d(net, [self.batch_size, 14, 14, 64], 4, 4, 2, 2, name='g_dc3'), is_training=is_training,\n",
    "                   scope='g_bn3'))\n",
    "\n",
    "            out = tf.nn.sigmoid(deconv2d(net, [self.batch_size, 28, 28, 1], 4, 4, 2, 2, name='g_dc4'))\n",
    "\n",
    "            return out\n",
    "\n",
    "    def build_model(self):\n",
    "        # some parameters\n",
    "        image_dims = [self.input_height, self.input_width, self.c_dim]\n",
    "        bs = self.batch_size\n",
    "\n",
    "        \"\"\" Graph Input \"\"\"\n",
    "        # images\n",
    "        self.inputs = tf.placeholder(tf.float32, [bs] + image_dims, name='real_images')\n",
    "\n",
    "        # noises\n",
    "        self.z = tf.placeholder(tf.float32, [bs, self.z_dim], name='z')\n",
    "\n",
    "        \"\"\" Loss Function \"\"\"\n",
    "\n",
    "        # output of D for real images\n",
    "        D_real, D_real_logits, _ = self.discriminator(self.inputs, is_training=True, reuse=False)\n",
    "\n",
    "        # output of D for fake images\n",
    "        G = self.generator(self.z, is_training=True, reuse=False)\n",
    "        D_fake, D_fake_logits, _ = self.discriminator(G, is_training=True, reuse=True)\n",
    "\n",
    "        # get loss for discriminator\n",
    "        d_loss_real = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real)))\n",
    "        d_loss_fake = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake)))\n",
    "\n",
    "        self.d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # get loss for generator\n",
    "        self.g_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake)))\n",
    "\n",
    "        \"\"\" Training \"\"\"\n",
    "        # divide trainable variables into a group for D and a group for G\n",
    "        t_vars = tf.trainable_variables()\n",
    "        d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "        g_vars = [var for var in t_vars if 'g_' in var.name]\n",
    "\n",
    "        # optimizers\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            self.d_optim = tf.train.AdamOptimizer(self.learning_rate, beta1=self.beta1) \\\n",
    "                      .minimize(self.d_loss, var_list=d_vars)\n",
    "            self.g_optim = tf.train.AdamOptimizer(self.learning_rate*5, beta1=self.beta1) \\\n",
    "                      .minimize(self.g_loss, var_list=g_vars)\n",
    "\n",
    "        \"\"\"\" Testing \"\"\"\n",
    "        # for test\n",
    "        self.fake_images = self.generator(self.z, is_training=False, reuse=True)\n",
    "\n",
    "        \"\"\" Summary \"\"\"\n",
    "        d_loss_real_sum = tf.summary.scalar(\"d_loss_real\", d_loss_real)\n",
    "        d_loss_fake_sum = tf.summary.scalar(\"d_loss_fake\", d_loss_fake)\n",
    "        d_loss_sum = tf.summary.scalar(\"d_loss\", self.d_loss)\n",
    "        g_loss_sum = tf.summary.scalar(\"g_loss\", self.g_loss)\n",
    "\n",
    "        # final summary operations\n",
    "        self.g_sum = tf.summary.merge([d_loss_fake_sum, g_loss_sum])\n",
    "        self.d_sum = tf.summary.merge([d_loss_real_sum, d_loss_sum])\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        # graph inputs for visualize training results\n",
    "        self.sample_z = np.random.uniform(-1, 1, size=(self.batch_size , self.z_dim))\n",
    "\n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # summary writer\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_name, self.sess.graph)\n",
    "\n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_counter / self.num_batches)\n",
    "            start_batch_id = checkpoint_counter - start_epoch * self.num_batches\n",
    "            counter = checkpoint_counter\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            start_batch_id = 0\n",
    "            counter = 1\n",
    "            print(\" [!] Load failed...\")\n",
    "\n",
    "        # loop for epoch\n",
    "        start_time = time.time()\n",
    "        for epoch in range(start_epoch, self.epoch):\n",
    "\n",
    "            # get batch data\n",
    "            for idx in range(start_batch_id, self.num_batches):\n",
    "                batch_images = self.data_X[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                batch_z = np.random.uniform(-1, 1, [self.batch_size, self.z_dim]).astype(np.float32)\n",
    "\n",
    "                # update D network\n",
    "                _, summary_str, d_loss = self.sess.run([self.d_optim, self.d_sum, self.d_loss],\n",
    "                                               feed_dict={self.inputs: batch_images, self.z: batch_z})\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                # update G network\n",
    "                _, summary_str, g_loss = self.sess.run([self.g_optim, self.g_sum, self.g_loss], feed_dict={self.z: batch_z})\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                # display training status\n",
    "                counter += 1\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f\" \\\n",
    "                      % (epoch, idx, self.num_batches, time.time() - start_time, d_loss, g_loss))\n",
    "\n",
    "                # save training results for every 300 steps\n",
    "                if np.mod(counter, 300) == 0:\n",
    "                    samples = self.sess.run(self.fake_images, feed_dict={self.z: self.sample_z})\n",
    "                    tot_num_samples = min(self.sample_num, self.batch_size)\n",
    "                    manifold_h = int(np.floor(np.sqrt(tot_num_samples)))\n",
    "                    manifold_w = int(np.floor(np.sqrt(tot_num_samples)))\n",
    "                    save_images(samples[:manifold_h * manifold_w, :, :, :], [manifold_h, manifold_w],\n",
    "                                './' + check_folder(self.result_dir + '/' + self.model_dir) + '/' + self.model_name + '_train_{:02d}_{:04d}.png'.format(\n",
    "                                    epoch, idx))\n",
    "\n",
    "            # After an epoch, start_batch_id is set to zero\n",
    "            # non-zero value is only for the first epoch after loading pre-trained model\n",
    "            start_batch_id = 0\n",
    "\n",
    "            # save model\n",
    "            self.save(self.checkpoint_dir, counter)\n",
    "\n",
    "            # show temporal results\n",
    "            self.visualize_results(epoch)\n",
    "\n",
    "        # save model for final step\n",
    "        self.save(self.checkpoint_dir, counter)\n",
    "\n",
    "    def visualize_results(self, epoch):\n",
    "        tot_num_samples = min(self.sample_num, self.batch_size)\n",
    "        image_frame_dim = int(np.floor(np.sqrt(tot_num_samples)))\n",
    "\n",
    "        \"\"\" random condition, random noise \"\"\"\n",
    "\n",
    "        z_sample = np.random.uniform(-1, 1, size=(self.batch_size, self.z_dim))\n",
    "\n",
    "        samples = self.sess.run(self.fake_images, feed_dict={self.z: z_sample})\n",
    "\n",
    "        save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
    "                    check_folder(self.result_dir + '/' + self.model_dir) + '/' + self.model_name + '_epoch%03d' % epoch + '_test_all_classes.png')\n",
    "\n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        return \"{}_{}_{}_{}\".format(\n",
    "            self.model_name, self.dataset_name,\n",
    "            self.batch_size, self.z_dim)\n",
    "\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir, self.model_name)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,os.path.join(checkpoint_dir, self.model_name+'.model'), global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir, self.model_name)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\",ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "            return True, counter\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "            return False, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/mnist\n",
      "reading gz data ./data/mnist/train-images-idx3-ubyte.gz\n",
      "reading gz data ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "reading gz data ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "reading gz data ./data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /workspaces/GANs/lib/ops.py:35: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspaces/GANs/lib/ops.py:36: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/codespace/.conda/envs/gan_env/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py:180: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /workspaces/GANs/lib/utils.py:59: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "discriminator/d_conv1/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]\n",
      "discriminator/d_conv1/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "discriminator/d_conv2/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]\n",
      "discriminator/d_conv2/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "discriminator/d_bn2/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "discriminator/d_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "discriminator/d_fc3/Matrix:0 (float32_ref 6272x1024) [6422528, bytes: 25690112]\n",
      "discriminator/d_fc3/bias:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "discriminator/d_bn3/beta:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "discriminator/d_bn3/gamma:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "discriminator/d_fc4/Matrix:0 (float32_ref 1024x1) [1024, bytes: 4096]\n",
      "discriminator/d_fc4/bias:0 (float32_ref 1) [1, bytes: 4]\n",
      "generator/g_fc1/Matrix:0 (float32_ref 62x1024) [63488, bytes: 253952]\n",
      "generator/g_fc1/bias:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "generator/g_bn1/beta:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "generator/g_bn1/gamma:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "generator/g_fc2/Matrix:0 (float32_ref 1024x6272) [6422528, bytes: 25690112]\n",
      "generator/g_fc2/bias:0 (float32_ref 6272) [6272, bytes: 25088]\n",
      "generator/g_bn2/beta:0 (float32_ref 6272) [6272, bytes: 25088]\n",
      "generator/g_bn2/gamma:0 (float32_ref 6272) [6272, bytes: 25088]\n",
      "generator/g_dc3/w:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]\n",
      "generator/g_dc3/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/g_dc4/w:0 (float32_ref 4x4x1x64) [1024, bytes: 4096]\n",
      "generator/g_dc4/biases:0 (float32_ref 1) [1, bytes: 4]\n",
      "Total size of variables: 13199362\n",
      "Total bytes of variables: 52797448\n",
      " [*] Reading checkpoints...\n",
      " [*] Failed to find a checkpoint\n",
      " [!] Load failed...\n",
      "Epoch: [ 0] [   0/ 546] time: 3.5890, d_loss: 1.43050408, g_loss: 0.76675248\n",
      "Epoch: [ 0] [   1/ 546] time: 4.0023, d_loss: 1.44335663, g_loss: 0.74238253\n",
      "Epoch: [ 0] [   2/ 546] time: 4.4453, d_loss: 1.41192365, g_loss: 0.73803186\n",
      "Epoch: [ 0] [   3/ 546] time: 4.9359, d_loss: 1.39893222, g_loss: 0.73354244\n",
      "Epoch: [ 0] [   4/ 546] time: 5.3660, d_loss: 1.40195179, g_loss: 0.73495948\n",
      "Epoch: [ 0] [   5/ 546] time: 5.8080, d_loss: 1.38073373, g_loss: 0.74330264\n",
      "Epoch: [ 0] [   6/ 546] time: 6.2376, d_loss: 1.37551475, g_loss: 0.74890822\n",
      "Epoch: [ 0] [   7/ 546] time: 6.6595, d_loss: 1.37206340, g_loss: 0.74707192\n",
      "Epoch: [ 0] [   8/ 546] time: 7.1593, d_loss: 1.37247038, g_loss: 0.74543864\n",
      "Epoch: [ 0] [   9/ 546] time: 7.5809, d_loss: 1.35929883, g_loss: 0.75137055\n",
      "Epoch: [ 0] [  10/ 546] time: 8.0344, d_loss: 1.36284423, g_loss: 0.75291783\n",
      "Epoch: [ 0] [  11/ 546] time: 8.4569, d_loss: 1.34886503, g_loss: 0.75981778\n",
      "Epoch: [ 0] [  12/ 546] time: 8.9534, d_loss: 1.33668590, g_loss: 0.76486051\n",
      "Epoch: [ 0] [  13/ 546] time: 9.4258, d_loss: 1.34292233, g_loss: 0.76228964\n",
      "Epoch: [ 0] [  14/ 546] time: 9.7825, d_loss: 1.33936763, g_loss: 0.75659209\n",
      "Epoch: [ 0] [  15/ 546] time: 10.0969, d_loss: 1.32156897, g_loss: 0.76662588\n",
      "Epoch: [ 0] [  16/ 546] time: 10.3819, d_loss: 1.34100866, g_loss: 0.76983202\n",
      "Epoch: [ 0] [  17/ 546] time: 10.7193, d_loss: 1.33093834, g_loss: 0.76663125\n",
      "Epoch: [ 0] [  18/ 546] time: 11.0519, d_loss: 1.31951857, g_loss: 0.77363729\n",
      "Epoch: [ 0] [  19/ 546] time: 11.4619, d_loss: 1.31994534, g_loss: 0.77683198\n",
      "Epoch: [ 0] [  20/ 546] time: 11.8029, d_loss: 1.31099403, g_loss: 0.77927822\n",
      "Epoch: [ 0] [  21/ 546] time: 12.1164, d_loss: 1.29748964, g_loss: 0.78834140\n",
      "Epoch: [ 0] [  22/ 546] time: 12.4368, d_loss: 1.29157758, g_loss: 0.79443604\n",
      "Epoch: [ 0] [  23/ 546] time: 12.7852, d_loss: 1.30107617, g_loss: 0.79141128\n",
      "Epoch: [ 0] [  24/ 546] time: 13.1354, d_loss: 1.28829026, g_loss: 0.79388416\n",
      "Epoch: [ 0] [  25/ 546] time: 13.4677, d_loss: 1.30692005, g_loss: 0.79074907\n",
      "Epoch: [ 0] [  26/ 546] time: 13.7925, d_loss: 1.27965832, g_loss: 0.79500097\n",
      "Epoch: [ 0] [  27/ 546] time: 14.1001, d_loss: 1.26637888, g_loss: 0.80430979\n",
      "Epoch: [ 0] [  28/ 546] time: 14.4082, d_loss: 1.26119566, g_loss: 0.81720519\n",
      "Epoch: [ 0] [  29/ 546] time: 14.7016, d_loss: 1.27237439, g_loss: 0.81491107\n",
      "Epoch: [ 0] [  30/ 546] time: 15.0471, d_loss: 1.25310743, g_loss: 0.81778508\n",
      "Epoch: [ 0] [  31/ 546] time: 15.3362, d_loss: 1.24466693, g_loss: 0.82251513\n",
      "Epoch: [ 0] [  32/ 546] time: 15.6565, d_loss: 1.26632500, g_loss: 0.82077807\n",
      "Epoch: [ 0] [  33/ 546] time: 15.9590, d_loss: 1.23976183, g_loss: 0.82247376\n",
      "Epoch: [ 0] [  34/ 546] time: 16.2742, d_loss: 1.23132503, g_loss: 0.83105659\n",
      "Epoch: [ 0] [  35/ 546] time: 16.5495, d_loss: 1.24942327, g_loss: 0.83148116\n",
      "Epoch: [ 0] [  36/ 546] time: 16.8518, d_loss: 1.21228123, g_loss: 0.84370852\n",
      "Epoch: [ 0] [  37/ 546] time: 17.1894, d_loss: 1.21111917, g_loss: 0.85039341\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-22d70f44fcec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# launch the graph in a session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" [*] Training finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ba38ab9a9c09>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;31m# update G network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_z\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/codespace/.conda/envs/gan_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/codespace/.conda/envs/gan_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/codespace/.conda/envs/gan_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/codespace/.conda/envs/gan_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/codespace/.conda/envs/gan_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/codespace/.conda/envs/gan_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    # declare instance for GAN\n",
    "    gan = GAN(sess, epoch=2, batch_size=128, z_dim=62, dataset_name='mnist', checkpoint_dir='checkpoint',\n",
    "        result_dir='results', log_dir='logs')\n",
    "    # build graph\n",
    "    gan.build_model()\n",
    "\n",
    "    # show network architecture\n",
    "    show_all_variables()\n",
    "\n",
    "    # launch the graph in a session\n",
    "    gan.train()\n",
    "    print(\" [*] Training finished!\")\n",
    "\n",
    "    # visualize learned generator\n",
    "    gan.visualize_results(args.epoch-1)\n",
    "    print(\" [*] Testing finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan_env",
   "language": "python",
   "name": "gan_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
